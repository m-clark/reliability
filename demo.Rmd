## Demonstrations

```{r demo_req, include=FALSE}
# possible radix bug where previous rmd output is not carrying over.
library(tidyverse)
library(visibly)
library(kableExtra)
kable_df <- function(..., digits=3) {
  kable(..., digits=digits) %>% 
    kable_styling(full_width = F)
}
```

### Preliminaries

Before diving into demonstration we will first describe the data and analytical approach.  Both observed and simulated data will be presented, followed by discussion of the analysis.

#### Data Description

##### Observed Data

The Big Five Inventory is a popular personality scale use in a wide variety of applications. For our example, we will have at our disposal 25 items corresponding to the five subscales- Agreeableness, Openness, Extroversion, Conscientiousness, and Neuroticism.  However, we will concern ourselves with the Neurtocism subscale specifically.  This particular data is available in the R package <span class="pack">psych</span>, and regards 2800 subjects as part of the Synthetic Aperture Personality Assessment (SAPA) web based personality assessment project.  The items are six-point scales ranging from 1 Very Inaccurate to six 6 Very Accurate, and are statements that may reflect the person's assessment of themselves.  The neuroticism items in particular are:

- N1: Get angry easily.
- N2: Get irritated easily.
- N3: Have frequent mood swings.
- N4: Often feel blue.
- N5: Panic easily.

More details can be found with the data object's (<span class="objclass">bfi</span>) associated helpfile.  The following shows how the data may be obtained. To make comparisons across the different approaches more easily comparable, I go ahead and remove the rows with missing data.

```{r bfi_neurot, echo=TRUE}
library(tidyverse)
library(psych)
neuroticism = bfi %>% 
  select(N1:N5) %>% 
  drop_na()
```

> Use only small fraction and compare to full in later discusion of sample size?

Basic descriptives and correlations are shown next. While there is some missing data, some reliability statistics will be based on pairwise correlations, and thus use all available information.  Some of the item correlations are not that strong, but this is a realistic situation for many data in social and related sciences.

```{r bfi_descriptive}
neuroticism %>% 
  tidyext::describe_all_num() %>% 
  select(-Min, -Max, -Q1, -Q3) %>% 
  kable_df()
```


```{r bfi_corr}
# visibly::corr_heat(cor(neuroticism, use = 'pair'), pal = 'acton', dir=1)

cor(neuroticism, use = 'pair') %>% 
  kable_df(digits=2)
```


##### Simulated/Ideal data

One of our investigations into reliability will involve what is commonly referred to as factor analysis.  Along with the observed data just described, the <span class="pack">psych</span> package additionally provides an easy means to simulate data with known factor structure.  We can specify the number of factors, loadings, number of items among other things.  Doing so will allow us to know what to expect from the factor analysis portion of the exploration, and explore uni- vs. multidimensional structure if desired.  As a starting point, we will simulate a data set regarding a <span class="emph">congeneric</span> factor model, one in which the factor structure regards just one latent variable underlying the items, but where the loadings for the observations need not be the same.  We will have six items for this data, with moderate to strong loadings between .4 and .7.

<aside>In the case where the loadings are equivalent, the model is referred to as <span class="emph">tau-equivalent</span>.</aside>


```{r sim_congeneric_create_data, echo=TRUE}
set.seed(123)
N = 1000
n_items = 6
loadings_congeneric = c(.4, .4, .5, .5, .6, .7)

cor_congeneric = sim.congeneric(loadings_congeneric, N = N)

data_congeneric = 
  mvtnorm::rmvnorm(n = N, 
                   mean = rep(0, n_items), 
                   sigma = cor_congeneric) %>% 
  as_data_frame() %>% 
  rename_all(str_replace, pattern = 'V', replacement = 'item_')
```

```{r inspect_data_congeneric}
data_congeneric %>% 
  as_data_frame() %>% 
  head() %>% 
  kable_df()
```



#### Analytical Approach

The analysis of the data will be conducted on both the observed and simulated data sets.  We will show three conceptual estimates of reliability, but, in addition, we will focus on the estimated uncertainty in those estimates.  Far too often reliability statistics are reported without any thought of the underlying models, or that there is possibly notable uncertainty in the estimate.  The three conceptual estimates include the most popular estimate of reliability, Coefficient $\alpha$, followed by two model-based approaches - generalizability theory and latent variable/factor analysis.



### Coefficient $\alpha$

<aside>Though it was developed independently a few years earlier by Guttman @guttman_basis_1945, coefficient $\alpha$ is often called Cronbach's $\alpha$. We prefer the more neutral terminology, in keeping with Cronbach's own wishes @cronbach_my_2004.</aside>

Coefficient $\alpha$ is one of the most popular measures of reliability.  Sometimes considered a measure of <span class="emph">internal consistency</span>, it is a function of the average covariance/correlation among the observations/items, the total variance of the test, as well as the number of items.  It is also interpreted as the average of all possible spit-half reliabilities. While it is descriptive in nature, it actually assumes a unidimensional factor structure as the underlying model representation, or in other words, that all the items correspond to the same underlying construct[^chron_app].  The standardized formula only requires the two values of the number of items $k$ and average inter-item correlation $\bar{r}$. 


$$\alpha = \frac{k\bar{r}}{1+(k-1)\bar{r}}$$

<aside>The raw score formula for $\alpha$ in terms of the average variance of the items $\bar{v}$ and average covariance $\bar{c}$ is: $$\alpha = \frac{k\bar{c}}{\bar{v}+(k-1)\bar{c}}$$</aside>

All else being equal, simply increasing the number of observations/items will give you a higher reliability.  In some contexts this may make sense, as the goal is to use an average or sum score, and we are interested in the reliability of that instead of any particular item. In other contexts such an estimate may overestimate the type of reliability we care about.

The following shows the results from the <span class="pack">psych</span> package.  In addition to both raw and standardized $\alpha$ measures, it also offers Guttman's lambda 6, a 'signal-to-noise' ratio and tohre info.  Shown are the alphas, absolute standard error, and average/median inter-item correlation.


```{r cronbach_base}
alpha_results = psych::alpha(neuroticism)

alpha_results %>% 
  magrittr::extract2('total') %>% 
  as_data_frame() %>% 
  rename(`Raw` = raw_alpha,
         Standardized = std.alpha,
         G6 = `G6(smc)`,
         `Avg. Inter-item cor` = average_r,
         ASE = ase,
         `Median r` = median_r) %>% 
  select(Raw, Standardized, `Avg. Inter-item cor`, `Median r`, ASE) %>% 
  kable_df(digits = 2)
```


These statistics show how $\alpha$ changes when the item is dropped.  We can see that items N1 through N3 are more useful measures, as dropping them would result in a significant drop in $\alpha$.

```{r cronbach_drop}
alpha_results %>% 
  magrittr::extract2('alpha.drop') %>% 
  as_data_frame() %>% 
  rename(`Raw` = raw_alpha,
         Standardized = std.alpha,
         G6 = `G6(smc)`,
         `Avg. Inter-item cor` = average_r,
         `Median r` = med.r,
         `Variance r` = var.r) %>% 
  select(Raw, Standardized, `Avg. Inter-item cor`, `Median r`) %>% 
  kable_df(digits = 2, caption = 'Reliability if the item is dropped.')
```


```{r neuroticism_gender_diff, eval=FALSE}
neuro_fem = neuroticism %>% filter(bfi$gender==1)
neuro_male = neuroticism %>% filter(bfi$gender==2)
neuro_fem_alpha = psych::alpha(neuro_fem)$total$std.alpha
neuro_male_alpha = psych::alpha(neuro_male)$total$std.alpha

alpha.ci(neuro_fem_alpha,
         n.obs = nrow(neuro_fem),
         n.var = ncol(neuro_fem)) %>% 
  rbind(alpha.ci(neuro_male_alpha,
         n.obs = nrow(neuro_male),
         n.var = ncol(neuro_male)) )
```



#### The Uncertainty of $\alpha$

One issue with Coefficient $\alpha$ is that the uncertainty in the estimate is rarely reported, even though it has been known for some time how to derive a confidence interval for it, and tools are readily available for producing it.  The <span class="pack">MBESS</span> package does this in a variety ways.  One uses an approach noted in Feldt et al. @feldt_statistical_1987, and which assumes fixed, rather than random, items and subjects. Another is based on a normal distribution approximation @van_zyl_distribution_2000. The other method is via the bootstrap @kelley_confidence_2016, calculating $\alpha$ for $R$ number of bootstrap resamples of the data. We will use the latter for our purposes. Results are shown below, with the bootstrapped value based on 1000 iterations. 


```{r load_MBESS}
library(MBESS)
```

```{r alpha_ci_boot, cache=TRUE}
set.seed(123)

# note that this is for raw, not standardized alpha
alpha_ci_boot   = ci.reliability(neuroticism, type = 'alpha', interval.type = 'perc', B = 1000)
alpha_ci_feldt  = ci.reliability(neuroticism, type = 'alpha', interval.type = 'feldt')
alpha_ci_normal = ci.reliability(neuroticism, type = 'alpha', interval.type = 'ml')
```

```{r alpha_ci_results}
# alpha_ci_results = list(alpha_ci_boot, alpha_ci_feldt, alpha_ci_normal) %>% 
#   map_df(as_tibble)

# alpha_ci_results = alpha_ci_results %>% 
#   select(interval.type, ci.lower, est, ci.upper) %>% 
#   mutate(interval.type = c('boot', 'feldt', 'normal')) %>% 
#   rename(` ` = interval.type,
#          LL  = ci.lower,
#          alpha = est,
#          UL = ci.upper)

alpha_ci_results = as.tibble(alpha_ci_boot)


alpha_ci_results = alpha_ci_results %>% 
  select(interval.type, ci.lower, est, ci.upper) %>% 
  mutate(interval.type = 'bootstrapped estimate') %>% 
  rename(` ` = interval.type,
         LL  = ci.lower,
         alpha = est,
         UL = ci.upper) 

alpha_ci_results  %>% 
  kable_df(digits = 4)
```


We can now see that our estimate of $\alpha$ would best be summarized as some value between `r arm::fround(alpha_ci_results$LL, 2)` and `r arm::fround(alpha_ci_results$UL,  2)`.



##### A Bayesian Approach

An alternative approach to estimating the uncertainty in $\alpha$ would be a Bayesian estimate.  We could estimate the value by first estimating the correlation matrix underlying the assumed multivariate normal distribution of the observations/items.  Thus the Bayesian $\alpha$ would be based on the posterior predictive distribution given the estimate of the correlation matrix @padilla_estimating_2011.  Alternatively, we could use a normal approximation for the distribution of the $\alpha$ itself, based on the estimated correlation matrix -@van_zyl_distribution_2000. Yet another approach would be based on a mixed model, calculating an <span class="emph">intra-class correlation coefficent</span> for a set number of items, as in Generalizability theory. We will save that for its own section later.  For now, we will use the first estimate.  More detail can be found in the supplemental materials.

```{r load_bayes_packages}
library(rstan); library(tidybayes)
```


```{r alpha_bayes_rstan, cache=TRUE, eval=T, cache.extra = tools::md5sum('code/alpha.stan')}
neuroticism_no_na = na.omit(scale(neuroticism))

data_list = list(X=neuroticism_no_na, 
                 N=nrow(neuroticism_no_na), 
                 p=ncol(neuroticism_no_na),
                 scale_flag = 0)

alpha_bayes = stan(file = 'code/alpha.stan', data = data_list, cores = 4, thin = 4)
```


```{r alpha_bayes_rstan_present}
# move to technical section
alpha_draws = spread_draws(alpha_bayes, alpha, alpha2, theta)

alpha_bayes_ci = 
  alpha_draws %>% 
  tidybayes::mean_qi() %>% 
  select(-.width, -.point, -.interval) %>% 
  tidyext::gather_multi(key     = value, 
                        values  = vars(`normal bayes`, `post. pred.`, `post. pred. non-normal`),
                        varlist = vars(c(alpha, alpha.lower, alpha.upper), starts_with('alpha2'), starts_with('theta'))) 

alpha_bayes_ci = as_data_frame(t(alpha_bayes_ci[,-1])) %>% 
  mutate(` ` = c('normal bayes', 'post. pred.', 'post. pred. non-normal')) %>% 
  select(` `, everything()) %>% 
  rename(alpha = V1,
         LL = V2,
         UL = V3) %>% 
  select(` `, LL, alpha, UL)

alpha_bayes_ci %>% 
  kable_df()
```

We can also view these estimates directly.  The normal approximation is wider than the other two.

```{r vis_alpha_bayes}
alpha_draws %>%
  gather(key=alpha, value=value, -.chain, -.iteration, -.draw) %>%
  qplot(x=value, geom = 'density', data=., 
        fill = alpha,
        color = alpha) +
  scico::scale_fill_scico_d(alpha = .5) +
  scico::scale_color_scico_d(alpha = .75) +
  theme_trueMinimal()
```

Here are all the estimates of uncertainty calculated.  For this amount of data it is not surprising that they are mostly in agreement, though the normal approximation may be a little wider.

```{r alpha_all}
alpha_ci_results %>% 
  bind_rows(alpha_bayes_ci) %>% 
  kable_df(digits = 2)

res = alpha_ci_results %>% 
  bind_rows(alpha_bayes_ci) %>% 
  rename(Coefficient = ` `,
         value = alpha,
         ui_l = LL,
         ui_u = UL) 

visibly:::plot_coefs(res,
                     palette = 'acton',
                     ref_line = alpha_results$total$raw_alpha,
                     trans = NULL) +
  labs(y='')
```



#### Simulated Data

The simulated data allows for us to have a more controlled exploration.  We know the items are multivariate normal and unidimensional, so this is where $\alpha$ shines as a measure of reliability.  However, one assumption with coefficient $\alpha$ is that the loadings for such a model are equivalent, and we know they aren't in this case.  As such, coefficient $\alpha$ is a an estimate of the lower bound of reliability. This scenario will serve as a comparison when we actually look at factor analytic approaches to reliability estimation later.  

For now, we'll skip the formality and cut right to the chase.  Here are all the previous estimates shown for this data set.  The $\alpha$ is lower for this particular data, but in general we're seeing the same thing.

```{r alpha_congeneric}
alpha_results_con = psych::alpha(data_congeneric)
```

```{r alpha_ci_boot_congeneric, cache=TRUE}
library(MBESS)

set.seed(123)

alpha_ci_boot_con   = ci.reliability(data_congeneric, type = 'alpha', interval.type = 'perc', B = 1000)
alpha_ci_feldt_con  = ci.reliability(data_congeneric, type = 'alpha', interval.type = 'feldt')
alpha_ci_normal_con = ci.reliability(data_congeneric, type = 'alpha', interval.type = 'ml')
```

```{r alpha_bayes_congeneric, cache=TRUE}
data_list = list(X=data_congeneric, 
                 N=nrow(data_congeneric), 
                 p=ncol(data_congeneric),
                 scale_flag = 0)

alpha_bayes_con = stan(file = 'code/alpha.stan', data = data_list, cores = 4, thin = 4)
```

```{r alpha_compare_congeneric}
alpha_ci_results_con = list(alpha_ci_boot_con, alpha_ci_feldt_con, alpha_ci_normal_con) %>% 
  map_df(as_tibble)


alpha_ci_results_con = alpha_ci_results_con %>% 
  select(interval.type, ci.lower, est, ci.upper) %>% 
  mutate(interval.type = c('boot', 'feldt', 'normal')) %>% 
  rename(` ` = interval.type,
         LL  = ci.lower,
         alpha = est,
         UL = ci.upper) 

alpha_draws = spread_draws(alpha_bayes_con, alpha, alpha2, theta)

alpha_bayes_ci_con = 
  alpha_draws %>% 
  tidybayes::mean_qi() %>% 
  select(-.width, -.point, -.interval) %>% 
  tidyext::gather_multi(key     = value, 
                        values  = vars(`Normal approx.`, `Post. Pred`, `Post. Pred. Non-normal`),
                        varlist = vars(c(alpha, alpha.lower, alpha.upper), starts_with('alpha2'), starts_with('theta'))) 

alpha_bayes_ci_con = as_data_frame(t(alpha_bayes_ci_con[,-1])) %>% 
  mutate(` ` = c('normal bayes', 'post. pred.', 'post. pred. non-normal')) %>% 
  select(` `, everything()) %>% 
  rename(alpha = V1,
         LL = V2,
         UL = V3) %>% 
  select(` `, LL, alpha, UL)

alpha_compare = alpha_ci_results_con %>% 
  bind_rows(alpha_bayes_ci_con)

alpha_compare %>% kable_df(digits = 2)

res = alpha_compare %>% 
  rename(Coefficient = ` `,
         value = alpha,
         ui_l = LL,
         ui_u = UL) 

visibly:::plot_coefs(res,
                     palette = 'bilbao',
                     ref_line = alpha_results_con$total$raw_alpha,
                     trans = NULL) +
  labs(y='')
```

#### Comparison to small sample


```{r alpha_small_sample, cache=TRUE}
neuro_10 = neuroticism %>% sample_frac(.1)
neuro_05 = neuroticism %>% sample_frac(.05)

alpha_ci_boot_small  = ci.reliability(neuro_10, type = 'alpha', interval.type = 'perc', B = 1000)

alpha_ci_boot_smaller  = ci.reliability(neuro_05, type = 'alpha', interval.type = 'perc', B = 1000)


alpha_ci_boot_small %>% 
  bind_rows(alpha_ci_boot_smaller) %>% 
  as_data_frame() %>% 
  select(interval.type, ci.lower, est, ci.upper) %>% 
  rename(
    Size = interval.type,
    LL  = ci.lower,
    alpha = est,
    UL = ci.upper)  %>% 
  mutate(Size = c('10%', '5%')) %>% 
  kable_df()
```



#### Multidimensional Scale

Notes: can't generalize from subscale to whole or vice versa, alpha < reliability in this setting @zinbarg_cronbachs_2005

```{r alpha_multi}
bfi_items_only = visibly::bfi
alpha_bfi_whole_scale = psych::alpha(bfi_items_only)

alpha_neuro = psych::alpha(neuroticism)
alpha_extra = psych::alpha(select(bfi_items_only, 
                                  starts_with('E', ignore.case = F)))
alpha_open  = psych::alpha(select(bfi_items_only, 
                                  starts_with('O', ignore.case = F)))
alpha_agree = psych::alpha(select(bfi_items_only, 
                                  starts_with('A', ignore.case = F)))
alpha_consc = psych::alpha(select(bfi_items_only, 
                                  starts_with('C', ignore.case = F)))

alpha_bfi_list = list(alpha_neuro, 
                      alpha_extra, 
                      alpha_open, 
                      alpha_agree, 
                      alpha_consc)

alpha_bfi_indiv_scale = alpha_bfi_list %>% 
  map(function(x) x$total$raw[1]) %>% 
  cbind() %>% 
  as.data.frame() %>% 
  mutate(Scale = c('Neuro', 'Extra', 'Open', 'Agree', 'Consc')) %>% 
  rename(Alpha = '.') %>% 
  select(Scale, Alpha) %>% 
  unnest()

alpha_bfi_whole_scale$total %>% 
  as_data_frame() %>% 
  rename(`Raw` = raw_alpha,
         Standardized = std.alpha,
         G6 = `G6(smc)`,
         `Avg. Inter-item cor` = average_r,
         ASE = ase,
         `Median r` = median_r) %>% 
  select(Raw, Standardized, `Avg. Inter-item cor`, `Median r`, ASE) %>% 
  kable_df(digits = 2)

alpha_bfi_indiv_scale %>% 
  kable_df()
```


#### Limitations of Coefficient $\alpha$

lower bound estimate

assumes single factor and equal loadings for items

merely adding items will improve


### Generalizability theory

So called <span class="emph">mixed effects models</span> are statistical models applicable to situations in which there is some dependency among observations in the data, where that correlation typically arises from the observations being clustered in some way. For example, it is quite common to have data in which we have repeated measurements of individuals, or cases in which the units of observation are otherwise grouped together, for example, students within school, or cities within geographic region.  This clustering can be hierarchical in nature (e.g. students within schools, schools within districts) or not (e.g. students and items on a test). While there are different ways to approach such a situation, mixed models are a powerful tool with which to do so.

Mixed models estimate the variance attributable to the various sources of dependency in the data.  Thus, aside from the usual regression output, we get a sense of variability due to individuals, species, surgical procedure, or whatever our grouping structure is.  In addition, we may estimate cluster-specific effects, which allow for increased predictive capability. Thus we may know the general trend across all individuals, but we can also allow each individual to have separate starting points and trends.  

In our data example, each person sees the five items, so those scores within each person are not independent. In other words, the multiple observations are clustered within individuals.  This becomes more clear when we consider our data in 'long' format, as follows.


```{r neuro_long_form, echo=TRUE}
neuroticism_long = neuroticism %>% 
  rowid_to_column(var='person') %>%          # create person id
  gather(key = item, value=score, -person)   # melt data into long format
```
```{r neuro_long_form_display}
neuroticism_long %>% 
  arrange(person, item) %>% 
  head( 15) %>%
  kable_df()
```


In similar fashion, we can see observations within items as being more correlated than observationa within other items. As such, in this case we can treat the person or item as a source of variance, or in other settings, even their interaction.

<span class="emph">Generalizability Theory</span> focuses on the variance components resulting from a linear mixed model.  Using the variance estimates we can obtain a measure of the reliability of a mean score for an individual across the multiple observations.  This reliability is defined as what proportion of the total variance is attributable to the particular source of variance, e.g. person, we wish to study.  For some grouping or clustering factor *g*, the simplest estimate of reliability ($\rho$) would be calculated as follows:


$$\rho = \frac{\sigma_g^2}{\sigma_g^2 +  \sigma_{residual}^2}$$

In more complicated circumstances

$$\rho = \frac{\sigma_g^2 + \sigma_{g*}^2}{\sigma_g^2 + \sigma_{g*}^2 + \sigma_{other}^2 + \sigma_{residual}^2}$$

In the above, *g\** refers to interactions of g with other other sources of variance, *other* refers to other still other sources of variance that don't include *g*, and then the *residual* variance is whatever else is not accounted for.

For simplicity we will focus on the person variance only.


```{r alpha_bayes_rstanarm, cache=TRUE, eval=FALSE}
# alpha as icc/generalizability statistic from g-theory
bfi_long = bfi_items_only %>% 
  gather(key = item, value = score) %>% 
  mutate(scale = str_sub(item, end=1))

library(rstanarm)
mixed_congeneric_bayes = stan_lmer(value ~ (1|subject), d_congeneric_long_std, cores=4, thin=4)
VarCorr(mixed_congeneric_bayes)

# note that Sigma is var intercept, while sigma is residual sd
var_comp_draws = spread_draws(mixed_congeneric_bayes, Sigma[subject:(Intercept),(Intercept)], sigma) %>% 
  mutate(sigma = sigma^2,
         alpha = Sigma/(Sigma + sigma/N_items))

# var_comp_draws %>% mean_qi(alpha)
print(alpha_bayes, par='alpha', digits=4)
```


#### The Uncertainty of Generalizability

#### Simulated Data
#### Multidimensional Scale
#### Limitations of G-theory



### Factor Analysis

omega (as generalization of alpha), ave

#### The Uncertainty of Factor Loadings

#### Simulated Data
#### Multidimensional Scale
#### Limitations of Factor Analytic Approach

[^chron_app]: Despite this, you will see it frequently reported in cases of multidimensional factor structure.