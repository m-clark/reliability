<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title>demo.utf8.md</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<script src="site_libs/kePrint-0.0.1/kePrint.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>



<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="fluid-row" id="header">




</div>


<div id="demonstrations" class="section level2">
<h2>Demonstrations</h2>
<div id="preliminaries" class="section level3">
<h3>Preliminaries</h3>
<p>Before diving into demonstration we will first describe the data and analytical approach. Both observed and simulated data will be presented, followed by discussion of the analysis.</p>
<div id="data-description" class="section level4">
<h4>Data Description</h4>
<div id="observed-data" class="section level5">
<h5>Observed Data</h5>
<p>The Big Five Inventory is a popular personality scale use in a wide variety of applications. For our example, we will have at our disposal 25 items corresponding to the five subscales- Agreeableness, Openness, Extroversion, Conscientiousness, and Neuroticism. However, we will concern ourselves with the Neurtocism subscale specifically. This particular data is available in the R package <span class="pack">psych</span>, and regards 2800 subjects as part of the Synthetic Aperture Personality Assessment (SAPA) web based personality assessment project. The items are six-point scales ranging from 1 Very Inaccurate to six 6 Very Accurate, and are statements that may reflect the person’s assessment of themselves. The neuroticism items in particular are:</p>
<ul>
<li>N1: Get angry easily.</li>
<li>N2: Get irritated easily.</li>
<li>N3: Have frequent mood swings.</li>
<li>N4: Often feel blue.</li>
<li>N5: Panic easily.</li>
</ul>
<p>More details can be found with the data object’s (<span class="objclass">bfi</span>) associated helpfile. The following shows how the data may be obtained.</p>
<pre class="r"><code>library(tidyverse)
library(psych)</code></pre>
<pre><code>## 
## Attaching package: &#39;psych&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:visibly&#39;:
## 
##     bfi</code></pre>
<pre><code>## The following objects are masked from &#39;package:ggplot2&#39;:
## 
##     %+%, alpha</code></pre>
<pre class="r"><code>neuroticism = select(bfi, N1:N5)</code></pre>
<p>Basic descriptives and correlations are shown next. While there is some missing data, some reliability statistics will be based on pairwise correlations, and thus use all available information. Some of the item correlations are not that strong, but this is a realistic situation for many data in social and related sciences.</p>
<pre class="r"><code>neuroticism %&gt;% 
  tidyext::describe_all_num() %&gt;% 
  select(-Min, -Max, -Q1, -Q3) %&gt;% 
  kable_df()</code></pre>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
Variable
</th>
<th style="text-align:right;">
N
</th>
<th style="text-align:right;">
Mean
</th>
<th style="text-align:right;">
SD
</th>
<th style="text-align:right;">
Median
</th>
<th style="text-align:right;">
Missing
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
N1
</td>
<td style="text-align:right;">
2778
</td>
<td style="text-align:right;">
2.93
</td>
<td style="text-align:right;">
1.57
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
22
</td>
</tr>
<tr>
<td style="text-align:left;">
N2
</td>
<td style="text-align:right;">
2779
</td>
<td style="text-align:right;">
3.51
</td>
<td style="text-align:right;">
1.53
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
21
</td>
</tr>
<tr>
<td style="text-align:left;">
N3
</td>
<td style="text-align:right;">
2789
</td>
<td style="text-align:right;">
3.22
</td>
<td style="text-align:right;">
1.60
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
11
</td>
</tr>
<tr>
<td style="text-align:left;">
N4
</td>
<td style="text-align:right;">
2764
</td>
<td style="text-align:right;">
3.19
</td>
<td style="text-align:right;">
1.57
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
36
</td>
</tr>
<tr>
<td style="text-align:left;">
N5
</td>
<td style="text-align:right;">
2771
</td>
<td style="text-align:right;">
2.97
</td>
<td style="text-align:right;">
1.62
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
29
</td>
</tr>
</tbody>
</table>
<pre class="r"><code># visibly::corr_heat(cor(neuroticism, use = &#39;pair&#39;), pal = &#39;acton&#39;, dir=1)

cor(neuroticism, use = &#39;pair&#39;) %&gt;% 
  kable_df(digits=2)</code></pre>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
N1
</th>
<th style="text-align:right;">
N2
</th>
<th style="text-align:right;">
N3
</th>
<th style="text-align:right;">
N4
</th>
<th style="text-align:right;">
N5
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
N1
</td>
<td style="text-align:right;">
1.00
</td>
<td style="text-align:right;">
0.71
</td>
<td style="text-align:right;">
0.56
</td>
<td style="text-align:right;">
0.40
</td>
<td style="text-align:right;">
0.38
</td>
</tr>
<tr>
<td style="text-align:left;">
N2
</td>
<td style="text-align:right;">
0.71
</td>
<td style="text-align:right;">
1.00
</td>
<td style="text-align:right;">
0.55
</td>
<td style="text-align:right;">
0.39
</td>
<td style="text-align:right;">
0.35
</td>
</tr>
<tr>
<td style="text-align:left;">
N3
</td>
<td style="text-align:right;">
0.56
</td>
<td style="text-align:right;">
0.55
</td>
<td style="text-align:right;">
1.00
</td>
<td style="text-align:right;">
0.52
</td>
<td style="text-align:right;">
0.43
</td>
</tr>
<tr>
<td style="text-align:left;">
N4
</td>
<td style="text-align:right;">
0.40
</td>
<td style="text-align:right;">
0.39
</td>
<td style="text-align:right;">
0.52
</td>
<td style="text-align:right;">
1.00
</td>
<td style="text-align:right;">
0.40
</td>
</tr>
<tr>
<td style="text-align:left;">
N5
</td>
<td style="text-align:right;">
0.38
</td>
<td style="text-align:right;">
0.35
</td>
<td style="text-align:right;">
0.43
</td>
<td style="text-align:right;">
0.40
</td>
<td style="text-align:right;">
1.00
</td>
</tr>
</tbody>
</table>
</div>
<div id="simulatedideal-data" class="section level5">
<h5>Simulated/Ideal data</h5>
<p>One of our investigations into reliability will involve what is commonly referred to as factor analysis. Along with the observed data just described, the <span class="pack">psych</span> package additionally provides an easy means to simulate data with known factor structure. We can specify the number of factors, loadings, number of items among other things. Doing so will allow us to know what to expect from the factor analysis portion of the exploration, and explore uni- vs. multidimensional structure if desired. As a starting point, we will simulate a <span class="emph">congeneric</span> data set, one in which the factor structure regards just one latent variable underlying the items. We will have six items for this data, with moderate to strong loadings between .4 and .7.</p>
<pre class="r"><code>set.seed(123)
N = 1000
n_items = 6
loadings_congeneric = c(.4, .4, .5, .5, .6, .7)

cor_congeneric = sim.congeneric(loadings_congeneric, N = N)

data_congeneric = 
  mvtnorm::rmvnorm(n = N, 
                   mean = rep(0, n_items), 
                   sigma = cor_congeneric) %&gt;% 
  as_data_frame() %&gt;% 
  rename_all(str_replace, pattern = &#39;V&#39;, replacement = &#39;item_&#39;)</code></pre>
<pre class="r"><code>data_congeneric %&gt;% 
  as_data_frame() %&gt;% 
  head() %&gt;% 
  kable_df()</code></pre>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
item_1
</th>
<th style="text-align:right;">
item_2
</th>
<th style="text-align:right;">
item_3
</th>
<th style="text-align:right;">
item_4
</th>
<th style="text-align:right;">
item_5
</th>
<th style="text-align:right;">
item_6
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
-1.158
</td>
<td style="text-align:right;">
0.592
</td>
<td style="text-align:right;">
2.019
</td>
<td style="text-align:right;">
0.797
</td>
<td style="text-align:right;">
1.869
</td>
<td style="text-align:right;">
0.329
</td>
</tr>
<tr>
<td style="text-align:right;">
-0.038
</td>
<td style="text-align:right;">
1.193
</td>
<td style="text-align:right;">
0.635
</td>
<td style="text-align:right;">
0.845
</td>
<td style="text-align:right;">
0.108
</td>
<td style="text-align:right;">
0.213
</td>
</tr>
<tr>
<td style="text-align:right;">
0.398
</td>
<td style="text-align:right;">
0.211
</td>
<td style="text-align:right;">
0.828
</td>
<td style="text-align:right;">
-0.138
</td>
<td style="text-align:right;">
-0.534
</td>
<td style="text-align:right;">
1.333
</td>
</tr>
<tr>
<td style="text-align:right;">
-0.712
</td>
<td style="text-align:right;">
0.194
</td>
<td style="text-align:right;">
-1.546
</td>
<td style="text-align:right;">
-0.551
</td>
<td style="text-align:right;">
1.944
</td>
<td style="text-align:right;">
0.084
</td>
</tr>
<tr>
<td style="text-align:right;">
0.839
</td>
<td style="text-align:right;">
0.521
</td>
<td style="text-align:right;">
-1.203
</td>
<td style="text-align:right;">
0.798
</td>
<td style="text-align:right;">
-0.822
</td>
<td style="text-align:right;">
0.606
</td>
</tr>
<tr>
<td style="text-align:right;">
-1.098
</td>
<td style="text-align:right;">
-0.047
</td>
<td style="text-align:right;">
0.429
</td>
<td style="text-align:right;">
-0.356
</td>
<td style="text-align:right;">
-0.116
</td>
<td style="text-align:right;">
0.364
</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="analytical-approach" class="section level4">
<h4>Analytical Approach</h4>
<p>The analysis of the data will be conducted on both the observed and simulated data sets. We will show three conceptual estimates of reliability, but, in addition, we will focus on the estimated uncertainty in those estimates. Far too often reliability statistics are reported without any thought of the underlying models, or that there is possibly notable uncertainty in the estimate. The three conceptual estimates include the most popular estimate of reliability, Cronbach’s <span class="math inline">\(\alpha\)</span>, followed by two model-based approaches - generalizability theory and latent variable/factor analysis.</p>
</div>
</div>
<div id="cronbachs-alpha" class="section level3">
<h3>Cronbach’s <span class="math inline">\(\alpha\)</span></h3>
<p>Cronbach’s <span class="math inline">\(\alpha\)</span> is one of the most popular measures of reliability. Sometimes considered an measure of <span class="emph">internal consistency</span>, it is a function of the average covariance/correlation among the observations/items, the total variance of the test, as well as the number of items. It is also interpreted as the mean of all possible spit-half reliabilities. While it is descriptive in nature, it assumes a unidimensional factor structure model representation, or in other words, that all the items correspond to the same underlying construct<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>. The standardized formula only requires the two values of the number of items <span class="math inline">\(k\)</span> and average inter-item correlation <span class="math inline">\(\bar{r}\)</span>.</p>
<p><span class="math display">\[\alpha = \frac{k\bar{r}}{1+(k-1)\bar{r}}\]</span></p>
<aside>
The raw score formula for <span class="math inline">\(\alpha\)</span> in terms of the average variance of the items <span class="math inline">\(\bar{v}\)</span> and average covariance <span class="math inline">\(\bar{c}\)</span> is: <span class="math display">\[\alpha = \frac{k\bar{c}}{\bar{v}+(k-1)\bar{c}}\]</span>
</aside>
<p>All else being equal, simply increasing the number of observations/items will give you a higher reliability. In some contexts this may make sense, as the goal is to use an average score, but in others it may not.</p>
<p>The following shows the results from the <span class="pack">psych</span> package. In addition to both raw and standardized <span class="math inline">\(\alpha\)</span> measures, it also offers Guttman’s lambda 6, a ‘signal-to-noise’ ratio and tohre info. Shown are the alphas, absolute standard error, and average/median inter-item correlation.</p>
<pre class="r"><code># psych::alpha(neuroticism) %&gt;% print(digits=3)
alpha_results = psych::alpha(neuroticism)


alpha_results %&gt;% 
  magrittr::extract2(&#39;total&#39;) %&gt;% 
  as_data_frame() %&gt;% 
  rename(`Raw` = raw_alpha,
         Standardized = std.alpha,
         G6 = `G6(smc)`,
         `Avg. Inter-item cor` = average_r,
         ASE = ase,
         `Median r` = median_r) %&gt;% 
  select(Raw, Standardized, `Avg. Inter-item cor`, `Median r`, ASE) %&gt;% 
  kable_df(digits = 2)</code></pre>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
Raw
</th>
<th style="text-align:right;">
Standardized
</th>
<th style="text-align:right;">
Avg. Inter-item cor
</th>
<th style="text-align:right;">
Median r
</th>
<th style="text-align:right;">
ASE
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:right;">
0.81
</td>
<td style="text-align:right;">
0.81
</td>
<td style="text-align:right;">
0.47
</td>
<td style="text-align:right;">
0.41
</td>
<td style="text-align:right;">
0.01
</td>
</tr>
</tbody>
</table>
<p>These statistics show how <span class="math inline">\(\alpha\)</span> changes when the item is dropped. We can see that items N1 through N3 are more useful measures, as dropping them would result in a significant drop in <span class="math inline">\(\alpha\)</span>.</p>
<pre class="r"><code>alpha_results %&gt;% 
  magrittr::extract2(&#39;alpha.drop&#39;) %&gt;% 
  as_data_frame() %&gt;% 
  rename(`Raw` = raw_alpha,
         Standardized = std.alpha,
         G6 = `G6(smc)`,
         `Avg. Inter-item cor` = average_r,
         `Median r` = med.r,
         `Variance r` = var.r) %&gt;% 
  select(Raw, Standardized, `Avg. Inter-item cor`, `Median r`) %&gt;% 
  kable_df(digits = 2, caption = &#39;Reliability if the item is dropped.&#39;)</code></pre>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
Reliability if the item is dropped.
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
Raw
</th>
<th style="text-align:right;">
Standardized
</th>
<th style="text-align:right;">
Avg. Inter-item cor
</th>
<th style="text-align:right;">
Median r
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
N1
</td>
<td style="text-align:right;">
0.76
</td>
<td style="text-align:right;">
0.76
</td>
<td style="text-align:right;">
0.44
</td>
<td style="text-align:right;">
0.41
</td>
</tr>
<tr>
<td style="text-align:left;">
N2
</td>
<td style="text-align:right;">
0.76
</td>
<td style="text-align:right;">
0.76
</td>
<td style="text-align:right;">
0.45
</td>
<td style="text-align:right;">
0.41
</td>
</tr>
<tr>
<td style="text-align:left;">
N3
</td>
<td style="text-align:right;">
0.76
</td>
<td style="text-align:right;">
0.76
</td>
<td style="text-align:right;">
0.44
</td>
<td style="text-align:right;">
0.39
</td>
</tr>
<tr>
<td style="text-align:left;">
N4
</td>
<td style="text-align:right;">
0.80
</td>
<td style="text-align:right;">
0.80
</td>
<td style="text-align:right;">
0.50
</td>
<td style="text-align:right;">
0.49
</td>
</tr>
<tr>
<td style="text-align:left;">
N5
</td>
<td style="text-align:right;">
0.81
</td>
<td style="text-align:right;">
0.81
</td>
<td style="text-align:right;">
0.52
</td>
<td style="text-align:right;">
0.53
</td>
</tr>
</tbody>
</table>
<pre class="r"><code>neuro_fem = neuroticism %&gt;% filter(bfi$gender==1)
neuro_male = neuroticism %&gt;% filter(bfi$gender==2)
neuro_fem_alpha = psych::alpha(neuro_fem)$total$std.alpha
neuro_male_alpha = psych::alpha(neuro_male)$total$std.alpha

alpha.ci(neuro_fem_alpha,
         n.obs = nrow(neuro_fem),
         n.var = ncol(neuro_fem)) %&gt;% 
  rbind(alpha.ci(neuro_male_alpha,
         n.obs = nrow(neuro_male),
         n.var = ncol(neuro_male)) )</code></pre>
<div id="the-uncertainty-of-alpha" class="section level4">
<h4>The Uncertainty of <span class="math inline">\(\alpha\)</span></h4>
<p>One issue with Cronbach’s <span class="math inline">\(\alpha\)</span> is that the uncertainty in the estimate is almost never reported, even though it has been known for decades how to derive a confidence interval for it, and tools are readily available for producing it. The <span class="pack"></span> package does this in two ways. One uses an approach noted in <span class="citation">@feldt_statistical_1987</span>, and which assumes fixed, rather than random, items and subjects. The other method is via the bootstrap, calculating <span class="math inline">\(\alpha\)</span> for <span class="math inline">\(R\)</span> number of bootstrap resamples of the data. Both results are shown below, with the bootstrapped value based on 1000 iterations.</p>
<pre class="r"><code>set.seed(123)

# note that the non-boot ci output is only generated as part of the print method
# for generic psych, and then cat(!), so there is no actual access to it.
alpha_ci = data_frame(LL = alpha_results$total$raw_alpha - 1.96 * alpha_results$total$ase, 
                      alpha = alpha_results$total$raw_alpha, 
                      UL = alpha_results$total$raw_alpha + 1.96 * alpha_results$total$ase)

alpha_boot_ci = alpha(neuroticism, n.iter = 1000)$boot.ci %&gt;% 
  t() %&gt;% 
  data.frame() %&gt;% 
  rename(LL = X2.5.,
         alpha = X50.,
         UL = X97.5.) 

alpha_ci %&gt;% 
  bind_rows(alpha_boot_ci) %&gt;% 
  mutate(` ` = c(&#39;alpha raw&#39;, &#39;alpha boot&#39;)) %&gt;% 
  select(` `, LL, alpha, UL) %&gt;% 
  kable_df(digits = 4)</code></pre>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
LL
</th>
<th style="text-align:right;">
alpha
</th>
<th style="text-align:right;">
UL
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
alpha raw
</td>
<td style="text-align:right;">
0.8030
</td>
<td style="text-align:right;">
0.8140
</td>
<td style="text-align:right;">
0.8250
</td>
</tr>
<tr>
<td style="text-align:left;">
alpha boot
</td>
<td style="text-align:right;">
0.8022
</td>
<td style="text-align:right;">
0.8139
</td>
<td style="text-align:right;">
0.8255
</td>
</tr>
</tbody>
</table>
<p>We can see that the bootstrapped interval is essentially the same, but in either case we can see that our estimate of <span class="math inline">\(\alpha\)</span> would best be summarized as some value between .80 and .83.</p>
<div id="a-bayesian-approach" class="section level5">
<h5>A Bayesian Approach</h5>
<p>An alternative approach to estimating the uncertainty in <span class="math inline">\(\alpha\)</span> would be a Bayesian estimate. We could estimate the value by first estimating the correlation matrix underlying the assumed multivariate normal distribution of the observations/items. Thus the Bayesian <span class="math inline">\(\alpha\)</span> would be based on the posterior predictive distribution given the estimate of the correlation matrix <span class="citation">@padilla_estimating_2011</span>. Alternatively, we could use a normal approximation for the distribution of the <span class="math inline">\(\alpha\)</span> itself, based on the estimated correlation matrix <span class="citation">@van_zyl_distribution_2000</span>. Yet another approach would be based on a mixed model, calculating an <span class="emph">intra-class correlation coefficent</span> for a set number of items, as in Generalizability theory. We will save that for the following section. More detail can be found in the supplemental materials.</p>
<pre class="r"><code>library(rstan); library(tidybayes)</code></pre>
<pre><code>## Loading required package: StanHeaders</code></pre>
<pre><code>## rstan (Version 2.18.2, GitRev: 2e1f913d3ca3)</code></pre>
<pre><code>## For execution on a local, multicore CPU with excess RAM we recommend calling
## options(mc.cores = parallel::detectCores()).
## To avoid recompilation of unchanged Stan programs, we recommend calling
## rstan_options(auto_write = TRUE)</code></pre>
<pre><code>## 
## Attaching package: &#39;rstan&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:psych&#39;:
## 
##     lookup</code></pre>
<pre><code>## The following object is masked from &#39;package:tidyr&#39;:
## 
##     extract</code></pre>
<pre><code>## NOTE: As of tidybayes version 1.0, several functions, arguments, and output column names
##       have undergone significant name changes in order to adopt a unified naming scheme.
##       See help(&#39;tidybayes-deprecated&#39;) for more information.</code></pre>
<pre class="r"><code>neuroticism_no_na = na.omit(scale(neuroticism))

data_list = list(X=neuroticism_no_na, 
                 N=nrow(neuroticism_no_na), 
                 p=ncol(neuroticism_no_na),
                 scale_flag = 0)

alpha_bayes = stan(file = &#39;code/alpha.stan&#39;, data = data_list, cores = 4, thin = 4)</code></pre>
<pre class="r"><code>alpha_draws = spread_draws(alpha_bayes, alpha, alpha2, theta)

alpha_bayes_ci = 
  alpha_draws %&gt;% 
  tidybayes::mean_qi() %&gt;% 
  select(-.width, -.point, -.interval) %&gt;% 
  tidyext::gather_multi(key     = value, 
                        values  = vars(`Normal approx.`, `Post. Pred`, `Post. Pred. Non-normal`),
                        varlist = vars(c(alpha, alpha.lower, alpha.upper), starts_with(&#39;alpha2&#39;), starts_with(&#39;theta&#39;))) 

alpha_bayes_ci = as_data_frame(t(alpha_bayes_ci[,-1])) %&gt;% 
  mutate(` ` = c(&#39;Normal approx.&#39;, &#39;Post. Pred&#39;, &#39;Post. Pred. Non-normal&#39;)) %&gt;% 
  select(` `, everything()) %&gt;% 
  rename(alpha = V1,
         LL = V2,
         UL = V3) %&gt;% 
  select(` `, LL, alpha, UL)

alpha_bayes_ci %&gt;% 
  kable_df()</code></pre>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
LL
</th>
<th style="text-align:right;">
alpha
</th>
<th style="text-align:right;">
UL
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Normal approx.
</td>
<td style="text-align:right;">
0.789
</td>
<td style="text-align:right;">
0.814
</td>
<td style="text-align:right;">
0.838
</td>
</tr>
<tr>
<td style="text-align:left;">
Post. Pred
</td>
<td style="text-align:right;">
0.805
</td>
<td style="text-align:right;">
0.814
</td>
<td style="text-align:right;">
0.822
</td>
</tr>
<tr>
<td style="text-align:left;">
Post. Pred. Non-normal
</td>
<td style="text-align:right;">
0.809
</td>
<td style="text-align:right;">
0.817
</td>
<td style="text-align:right;">
0.825
</td>
</tr>
</tbody>
</table>
<p>We can also view these estimates directly. The normal approximation is wider than the other two.</p>
<pre class="r"><code>alpha_draws %&gt;%
  gather(key=alpha, value=value, -.chain, -.iteration, -.draw) %&gt;%
  qplot(x=value, geom = &#39;density&#39;, data=., 
        fill = alpha,
        color = alpha) +
  scico::scale_fill_scico_d(alpha = .5) +
  scico::scale_color_scico_d(alpha = .75) +
  theme_trueMinimal()</code></pre>
<p><img src="demo_files/figure-html/vis_alpha_bayes-1.png" width="672" /></p>
<p>Here are all the estimates of uncertainty calculated. For this amount of data it is not surprising that they are mostly in agreement, though the normal approximation may be a little wider.</p>
<pre class="r"><code>alpha_ci %&gt;% 
  bind_rows(alpha_boot_ci) %&gt;% 
  mutate(` ` = c(&#39;alpha raw&#39;, &#39;alpha boot&#39;)) %&gt;% 
  select(` `, LL, alpha, UL) %&gt;% 
  bind_rows(alpha_bayes_ci) %&gt;% 
  kable_df(digits = 2)</code></pre>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
LL
</th>
<th style="text-align:right;">
alpha
</th>
<th style="text-align:right;">
UL
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
alpha raw
</td>
<td style="text-align:right;">
0.80
</td>
<td style="text-align:right;">
0.81
</td>
<td style="text-align:right;">
0.82
</td>
</tr>
<tr>
<td style="text-align:left;">
alpha boot
</td>
<td style="text-align:right;">
0.80
</td>
<td style="text-align:right;">
0.81
</td>
<td style="text-align:right;">
0.83
</td>
</tr>
<tr>
<td style="text-align:left;">
Normal approx.
</td>
<td style="text-align:right;">
0.79
</td>
<td style="text-align:right;">
0.81
</td>
<td style="text-align:right;">
0.84
</td>
</tr>
<tr>
<td style="text-align:left;">
Post. Pred
</td>
<td style="text-align:right;">
0.81
</td>
<td style="text-align:right;">
0.81
</td>
<td style="text-align:right;">
0.82
</td>
</tr>
<tr>
<td style="text-align:left;">
Post. Pred. Non-normal
</td>
<td style="text-align:right;">
0.81
</td>
<td style="text-align:right;">
0.82
</td>
<td style="text-align:right;">
0.82
</td>
</tr>
</tbody>
</table>
<pre class="r"><code>res = alpha_ci %&gt;% 
  bind_rows(alpha_boot_ci) %&gt;% 
  mutate(` ` = c(&#39;raw&#39;, &#39;boot&#39;)) %&gt;% 
  select(` `, LL, alpha, UL) %&gt;% 
  bind_rows(alpha_bayes_ci) %&gt;% 
  rename(Coefficient = ` `,
         value = alpha,
         ui_l = LL,
         ui_u = UL) 

visibly:::plot_coefs(res,
                     palette = &#39;acton&#39;,
                     ref_line = alpha_ci$alpha,
                     trans = NULL) +
  labs(y=&#39;&#39;)</code></pre>
<p><img src="demo_files/figure-html/alpha_all-1.png" width="672" /></p>
</div>
</div>
<div id="simulated-data" class="section level4">
<h4>Simulated Data</h4>
<p>The simulated data allows for us to have a more controlled exploration. We know the items are multivariate normal and unidimensional, so this is where <span class="math inline">\(\alpha\)</span> shines as a measure of reliability. We’ll skip the formality and cut right to the chase. Here are all the previous estimates for this data set. The <span class="math inline">\(\alpha\)</span> is</p>
<pre class="r"><code>alpha_results = psych::alpha(data_congeneric)
alpha_ci = data_frame(LL = alpha_results$total$raw_alpha - 1.96 * alpha_results$total$ase, 
                      alpha = alpha_results$total$raw_alpha, 
                      UL = alpha_results$total$raw_alpha + 1.96 * alpha_results$total$ase)

alpha_boot_ci = alpha(data_congeneric, n.iter = 1000)$boot.ci %&gt;% 
  t() %&gt;% 
  data.frame() %&gt;% 
  rename(LL = X2.5.,
         alpha = X50.,
         UL = X97.5.) 

data_list = list(X=data_congeneric, 
                 N=nrow(data_congeneric), 
                 p=ncol(data_congeneric),
                 scale_flag = 0)

alpha_bayes = stan(file = &#39;code/alpha.stan&#39;, data = data_list, cores = 4, thin = 4)</code></pre>
<pre class="r"><code>alpha_draws = spread_draws(alpha_bayes, alpha, alpha2, theta)

alpha_bayes_ci = 
  alpha_draws %&gt;% 
  tidybayes::mean_qi() %&gt;% 
  select(-.width, -.point, -.interval) %&gt;% 
  tidyext::gather_multi(key     = value, 
                        values  = vars(`Normal approx.`, `Post. Pred`, `Post. Pred. Non-normal`),
                        varlist = vars(c(alpha, alpha.lower, alpha.upper), starts_with(&#39;alpha2&#39;), starts_with(&#39;theta&#39;))) 

alpha_bayes_ci = as_data_frame(t(alpha_bayes_ci[,-1])) %&gt;% 
  mutate(` ` = c(&#39;Normal approx.&#39;, &#39;Post. Pred&#39;, &#39;Post. Pred. Non-normal&#39;)) %&gt;% 
  select(` `, everything()) %&gt;% 
  rename(alpha = V1,
         LL = V2,
         UL = V3) %&gt;% 
  select(` `, LL, alpha, UL)

alpha_compare = alpha_ci %&gt;% 
  bind_rows(alpha_boot_ci) %&gt;% 
  mutate(` ` = c(&#39;alpha raw&#39;, &#39;alpha boot&#39;)) %&gt;% 
  select(` `, LL, alpha, UL) %&gt;% 
  bind_rows(alpha_bayes_ci)

alpha_compare %&gt;% kable_df(digits = 2)</code></pre>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
LL
</th>
<th style="text-align:right;">
alpha
</th>
<th style="text-align:right;">
UL
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
alpha raw
</td>
<td style="text-align:right;">
0.69
</td>
<td style="text-align:right;">
0.72
</td>
<td style="text-align:right;">
0.75
</td>
</tr>
<tr>
<td style="text-align:left;">
alpha boot
</td>
<td style="text-align:right;">
0.69
</td>
<td style="text-align:right;">
0.72
</td>
<td style="text-align:right;">
0.74
</td>
</tr>
<tr>
<td style="text-align:left;">
Normal approx.
</td>
<td style="text-align:right;">
0.67
</td>
<td style="text-align:right;">
0.72
</td>
<td style="text-align:right;">
0.77
</td>
</tr>
<tr>
<td style="text-align:left;">
Post. Pred
</td>
<td style="text-align:right;">
0.69
</td>
<td style="text-align:right;">
0.72
</td>
<td style="text-align:right;">
0.74
</td>
</tr>
<tr>
<td style="text-align:left;">
Post. Pred. Non-normal
</td>
<td style="text-align:right;">
0.71
</td>
<td style="text-align:right;">
0.73
</td>
<td style="text-align:right;">
0.75
</td>
</tr>
</tbody>
</table>
<pre class="r"><code>res = alpha_compare %&gt;% 
  rename(Coefficient = ` `,
         value = alpha,
         ui_l = LL,
         ui_u = UL) 
visibly:::plot_coefs(res,
                     palette = &#39;bilbao&#39;,
                     ref_line = alpha_ci$alpha,
                     trans = NULL) +
  labs(y=&#39;&#39;)</code></pre>
<p><img src="demo_files/figure-html/alpha_congeneric_compare-1.png" width="672" /></p>
</div>
</div>
<div id="generalizability-theory" class="section level3">
<h3>Generalizability theory</h3>
<p><span class="math display">\[\rho = \frac{\sigma_g^2}{\sigma_g^2 + \sigma^2}\]</span></p>
<pre class="r"><code># alpha as icc/generalizability statistic from g-theory

library(rstanarm)
mixed_congeneric_bayes = stan_lmer(value ~ (1|subject), d_congeneric_long_std, cores=4, thin=4)
VarCorr(mixed_congeneric_bayes)

# note that Sigma is var intercept, while sigma is residual sd
var_comp_draws = spread_draws(mixed_congeneric_bayes, Sigma[subject:(Intercept),(Intercept)], sigma) %&gt;% 
  mutate(sigma = sigma^2,
         alpha = Sigma/(Sigma + sigma/N_items))

# var_comp_draws %&gt;% mean_qi(alpha)
print(alpha_bayes, par=&#39;alpha&#39;, digits=4)</code></pre>
<div id="the-uncertainty-of-generalizability" class="section level4">
<h4>The Uncertainty of Generalizability</h4>
</div>
<div id="simulated-data-1" class="section level4">
<h4>Simulated Data</h4>
</div>
</div>
<div id="factor-analysis" class="section level3">
<h3>Factor Analysis</h3>
<p>omega, ave</p>
<div id="the-uncertainty-of-factor-loadings" class="section level4">
<h4>The Uncertainty of Factor Loadings</h4>
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Despite this, you will see it frequently reported in cases of multidimensional factor structure.<a href="#fnref1" class="footnote-back">↩</a></p></li>
</ol>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
