<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title>demo.utf8.md</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<script src="site_libs/kePrint-0.0.1/kePrint.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>



<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="fluid-row" id="header">




</div>


<div id="demonstrations" class="section level2">
<h2>Demonstrations</h2>
<div id="preliminaries" class="section level3">
<h3>Preliminaries</h3>
<p>Before diving into demonstration we will first describe the data and analytical approach. Both observed and simulated data will be presented, followed by discussion of the analysis.</p>
<div id="data-description" class="section level4">
<h4>Data Description</h4>
<div id="observed-data" class="section level5">
<h5>Observed Data</h5>
<p>The Big Five Inventory is a popular personality scale use in a wide variety of applications. For our example, we will have at our disposal 25 items corresponding to the five subscales- Agreeableness, Openness, Extroversion, Conscientiousness, and Neuroticism. However, we will concern ourselves with the Neurtocism subscale specifically. This particular data is available in the R package <span class="pack">psych</span>, and regards 2800 subjects as part of the Synthetic Aperture Personality Assessment (SAPA) web based personality assessment project. The items are six-point scales ranging from 1 Very Inaccurate to six 6 Very Accurate, and are statements that may reflect the person’s assessment of themselves. The neuroticism items in particular are:</p>
<ul>
<li>N1: Get angry easily.</li>
<li>N2: Get irritated easily.</li>
<li>N3: Have frequent mood swings.</li>
<li>N4: Often feel blue.</li>
<li>N5: Panic easily.</li>
</ul>
<p>More details can be found with the data object’s (<span class="objclass">bfi</span>) associated helpfile. The following shows how the data may be obtained. To make comparisons across the different approaches more easily comparable, I go ahead and remove the rows with missing data.</p>
<pre class="r"><code>library(tidyverse)
library(psych)</code></pre>
<pre><code>## 
## Attaching package: &#39;psych&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:visibly&#39;:
## 
##     bfi</code></pre>
<pre><code>## The following objects are masked from &#39;package:ggplot2&#39;:
## 
##     %+%, alpha</code></pre>
<pre class="r"><code>neuroticism = bfi %&gt;% 
  select(N1:N5) %&gt;% 
  drop_na()</code></pre>
<blockquote>
<p>Use only small fraction and compare to full in later discusion of sample size?</p>
</blockquote>
<p>Basic descriptives and correlations are shown next. While there is some missing data, some reliability statistics will be based on pairwise correlations, and thus use all available information. Some of the item correlations are not that strong, but this is a realistic situation for many data in social and related sciences.</p>
<pre class="r"><code>neuroticism %&gt;% 
  tidyext::describe_all_num() %&gt;% 
  select(-Min, -Max, -Q1, -Q3) %&gt;% 
  kable_df()</code></pre>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
Variable
</th>
<th style="text-align:right;">
N
</th>
<th style="text-align:right;">
Mean
</th>
<th style="text-align:right;">
SD
</th>
<th style="text-align:right;">
Median
</th>
<th style="text-align:right;">
Missing
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
N1
</td>
<td style="text-align:right;">
2694
</td>
<td style="text-align:right;">
2.93
</td>
<td style="text-align:right;">
1.57
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:left;">
N2
</td>
<td style="text-align:right;">
2694
</td>
<td style="text-align:right;">
3.51
</td>
<td style="text-align:right;">
1.53
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:left;">
N3
</td>
<td style="text-align:right;">
2694
</td>
<td style="text-align:right;">
3.22
</td>
<td style="text-align:right;">
1.60
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:left;">
N4
</td>
<td style="text-align:right;">
2694
</td>
<td style="text-align:right;">
3.19
</td>
<td style="text-align:right;">
1.57
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:left;">
N5
</td>
<td style="text-align:right;">
2694
</td>
<td style="text-align:right;">
2.97
</td>
<td style="text-align:right;">
1.62
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
0
</td>
</tr>
</tbody>
</table>
<pre class="r"><code># visibly::corr_heat(cor(neuroticism, use = &#39;pair&#39;), pal = &#39;acton&#39;, dir=1)

cor(neuroticism, use = &#39;pair&#39;) %&gt;% 
  kable_df(digits=2)</code></pre>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
N1
</th>
<th style="text-align:right;">
N2
</th>
<th style="text-align:right;">
N3
</th>
<th style="text-align:right;">
N4
</th>
<th style="text-align:right;">
N5
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
N1
</td>
<td style="text-align:right;">
1.00
</td>
<td style="text-align:right;">
0.71
</td>
<td style="text-align:right;">
0.56
</td>
<td style="text-align:right;">
0.40
</td>
<td style="text-align:right;">
0.38
</td>
</tr>
<tr>
<td style="text-align:left;">
N2
</td>
<td style="text-align:right;">
0.71
</td>
<td style="text-align:right;">
1.00
</td>
<td style="text-align:right;">
0.55
</td>
<td style="text-align:right;">
0.39
</td>
<td style="text-align:right;">
0.35
</td>
</tr>
<tr>
<td style="text-align:left;">
N3
</td>
<td style="text-align:right;">
0.56
</td>
<td style="text-align:right;">
0.55
</td>
<td style="text-align:right;">
1.00
</td>
<td style="text-align:right;">
0.52
</td>
<td style="text-align:right;">
0.43
</td>
</tr>
<tr>
<td style="text-align:left;">
N4
</td>
<td style="text-align:right;">
0.40
</td>
<td style="text-align:right;">
0.39
</td>
<td style="text-align:right;">
0.52
</td>
<td style="text-align:right;">
1.00
</td>
<td style="text-align:right;">
0.40
</td>
</tr>
<tr>
<td style="text-align:left;">
N5
</td>
<td style="text-align:right;">
0.38
</td>
<td style="text-align:right;">
0.35
</td>
<td style="text-align:right;">
0.43
</td>
<td style="text-align:right;">
0.40
</td>
<td style="text-align:right;">
1.00
</td>
</tr>
</tbody>
</table>
</div>
<div id="simulatedideal-data" class="section level5">
<h5>Simulated/Ideal data</h5>
<p>One of our investigations into reliability will involve what is commonly referred to as factor analysis. Along with the observed data just described, the <span class="pack">psych</span> package additionally provides an easy means to simulate data with known factor structure. We can specify the number of factors, loadings, number of items among other things. Doing so will allow us to know what to expect from the factor analysis portion of the exploration, and explore uni- vs. multidimensional structure if desired. As a starting point, we will simulate a data set regarding a <span class="emph">congeneric</span> factor model, one in which the factor structure regards just one latent variable underlying the observed items, but where the loadings for the observations need not be the same. We will have six items for this data, with moderate to strong loadings between .4 and .7.</p>
<aside>
In the case where the loadings are equivalent, the model is referred to as <span class="emph">tau-equivalent</span>.
</aside>
<pre class="r"><code>set.seed(123)
N = 1000
n_items = 6
loadings_congeneric = c(.4, .4, .5, .5, .6, .7)

cor_congeneric = sim.congeneric(loadings_congeneric, N = N)

data_congeneric = 
  mvtnorm::rmvnorm(n = N, 
                   mean = rep(0, n_items), 
                   sigma = cor_congeneric) %&gt;% 
  as_data_frame() %&gt;% 
  rename_all(str_replace, pattern = &#39;V&#39;, replacement = &#39;item_&#39;)</code></pre>
<pre><code>## Warning: `as_data_frame()` is deprecated, use `as_tibble()` (but mind the new semantics).
## This warning is displayed once per session.</code></pre>
<pre class="r"><code>data_congeneric %&gt;% 
  as_data_frame() %&gt;% 
  head() %&gt;% 
  kable_df()</code></pre>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
item_1
</th>
<th style="text-align:right;">
item_2
</th>
<th style="text-align:right;">
item_3
</th>
<th style="text-align:right;">
item_4
</th>
<th style="text-align:right;">
item_5
</th>
<th style="text-align:right;">
item_6
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
-1.158
</td>
<td style="text-align:right;">
0.592
</td>
<td style="text-align:right;">
2.019
</td>
<td style="text-align:right;">
0.797
</td>
<td style="text-align:right;">
1.869
</td>
<td style="text-align:right;">
0.329
</td>
</tr>
<tr>
<td style="text-align:right;">
-0.038
</td>
<td style="text-align:right;">
1.193
</td>
<td style="text-align:right;">
0.635
</td>
<td style="text-align:right;">
0.845
</td>
<td style="text-align:right;">
0.108
</td>
<td style="text-align:right;">
0.213
</td>
</tr>
<tr>
<td style="text-align:right;">
0.398
</td>
<td style="text-align:right;">
0.211
</td>
<td style="text-align:right;">
0.828
</td>
<td style="text-align:right;">
-0.138
</td>
<td style="text-align:right;">
-0.534
</td>
<td style="text-align:right;">
1.333
</td>
</tr>
<tr>
<td style="text-align:right;">
-0.712
</td>
<td style="text-align:right;">
0.194
</td>
<td style="text-align:right;">
-1.546
</td>
<td style="text-align:right;">
-0.551
</td>
<td style="text-align:right;">
1.944
</td>
<td style="text-align:right;">
0.084
</td>
</tr>
<tr>
<td style="text-align:right;">
0.839
</td>
<td style="text-align:right;">
0.521
</td>
<td style="text-align:right;">
-1.203
</td>
<td style="text-align:right;">
0.798
</td>
<td style="text-align:right;">
-0.822
</td>
<td style="text-align:right;">
0.606
</td>
</tr>
<tr>
<td style="text-align:right;">
-1.098
</td>
<td style="text-align:right;">
-0.047
</td>
<td style="text-align:right;">
0.429
</td>
<td style="text-align:right;">
-0.356
</td>
<td style="text-align:right;">
-0.116
</td>
<td style="text-align:right;">
0.364
</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="analytical-approach" class="section level4">
<h4>Analytical Approach</h4>
<p>The analysis of the data will be conducted on both the observed and simulated data sets. We will show three conceptual estimates of reliability, but, in addition, we will focus on the estimated uncertainty in those estimates. Far too often reliability statistics are reported without any thought of the underlying models, or that there is possibly notable uncertainty in the estimate. The three conceptual estimates include the most popular estimate of reliability, Coefficient <span class="math inline">\(\alpha\)</span>, followed by two model-based approaches - generalizability theory and latent variable/factor analysis.</p>
</div>
</div>
<div id="coefficient-alpha" class="section level3">
<h3>Coefficient <span class="math inline">\(\alpha\)</span></h3>
<p>Coefficient <span class="math inline">\(\alpha\)</span> is one of the most popular measures of reliability. Sometimes considered a measure of <span class="emph">internal consistency</span>, it is a function of the average covariance/correlation among the observations/items, the total variance of the test, as well as the number of items. It is also interpreted as the average of all possible spit-half reliabilities. While it is descriptive in nature, it actually assumes a unidimensional factor structure as the underlying model representation, or in other words, that all the items correspond to the same underlying construct<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>. The standardized formula only requires the two values of the number of items <span class="math inline">\(k\)</span> and average inter-item correlation <span class="math inline">\(\bar{r}\)</span>.</p>
<aside>
Though it was developed independently a few years earlier by Guttman <span class="citation">@guttman_basis_1945</span>, coefficient <span class="math inline">\(\alpha\)</span> is often called Cronbach’s <span class="math inline">\(\alpha\)</span>. We prefer the more neutral terminology, in keeping with Cronbach’s own wishes <span class="citation">@cronbach_my_2004</span>.
</aside>
<p><span class="math display">\[\alpha = \frac{k\bar{r}}{1+(k-1)\bar{r}}\]</span></p>
<p>All else being equal, simply increasing the number of observations/items will give you a higher reliability. In some contexts this may make sense, as the goal is to use an average or sum score, and we are interested in the reliability of that instead of any particular item. In other contexts such an estimate may overestimate the type of reliability we care about.</p>
<aside>
The raw score formula for <span class="math inline">\(\alpha\)</span> in terms of the average variance of the items <span class="math inline">\(\bar{v}\)</span> and average covariance <span class="math inline">\(\bar{c}\)</span> is: <span class="math display">\[\alpha = \frac{k\bar{c}}{\bar{v}+(k-1)\bar{c}}\]</span>
</aside>
<p>The following shows the results from the <span class="pack">psych</span> package. In addition to both raw and standardized <span class="math inline">\(\alpha\)</span> measures, it also offers Guttman’s lambda 6, a ‘signal-to-noise’ ratio and tohre info. Shown are the <span class="math inline">\(\alpha\)</span> values, absolute standard error, and average/median inter-item correlation.</p>
<pre class="r"><code>alpha_results = psych::alpha(neuroticism)

alpha_results %&gt;% 
  magrittr::extract2(&#39;total&#39;) %&gt;% 
  as_tibble() %&gt;% 
  rownames_to_column(&#39;Item&#39;) %&gt;% 
  rename(`Raw` = raw_alpha,
         Standardized = std.alpha,
         G6 = `G6(smc)`,
         `Avg. Inter-item cor` = average_r,
         ASE = ase,
         `Median r` = median_r) %&gt;% 
  select(Raw, Standardized, `Avg. Inter-item cor`, `Median r`, ASE) %&gt;% 
  kable_df(digits = 2)</code></pre>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
Raw
</th>
<th style="text-align:right;">
Standardized
</th>
<th style="text-align:right;">
Avg. Inter-item cor
</th>
<th style="text-align:right;">
Median r
</th>
<th style="text-align:right;">
ASE
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
0.81
</td>
<td style="text-align:right;">
0.81
</td>
<td style="text-align:right;">
0.47
</td>
<td style="text-align:right;">
0.41
</td>
<td style="text-align:right;">
0.01
</td>
</tr>
</tbody>
</table>
<p>These statistics show how <span class="math inline">\(\alpha\)</span> changes when the item is dropped. We can see that items N1 through N3 are more useful measures, as dropping them would result in a significant drop in <span class="math inline">\(\alpha\)</span>.</p>
<pre class="r"><code>alpha_results %&gt;% 
  magrittr::extract2(&#39;alpha.drop&#39;) %&gt;% 
  as_data_frame() %&gt;% 
  rename(`Raw` = raw_alpha,
         Standardized = std.alpha,
         G6 = `G6(smc)`,
         `Avg. Inter-item cor` = average_r,
         `Median r` = med.r,
         `Variance r` = var.r) %&gt;% 
  select(Raw, Standardized, `Avg. Inter-item cor`, `Median r`) %&gt;% 
  kable_df(digits = 2, caption = &#39;Reliability if the item is dropped.&#39;)</code></pre>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
Reliability if the item is dropped.
</caption>
<thead>
<tr>
<th style="text-align:right;">
Raw
</th>
<th style="text-align:right;">
Standardized
</th>
<th style="text-align:right;">
Avg. Inter-item cor
</th>
<th style="text-align:right;">
Median r
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
0.76
</td>
<td style="text-align:right;">
0.76
</td>
<td style="text-align:right;">
0.44
</td>
<td style="text-align:right;">
0.41
</td>
</tr>
<tr>
<td style="text-align:right;">
0.76
</td>
<td style="text-align:right;">
0.76
</td>
<td style="text-align:right;">
0.45
</td>
<td style="text-align:right;">
0.41
</td>
</tr>
<tr>
<td style="text-align:right;">
0.75
</td>
<td style="text-align:right;">
0.76
</td>
<td style="text-align:right;">
0.44
</td>
<td style="text-align:right;">
0.39
</td>
</tr>
<tr>
<td style="text-align:right;">
0.79
</td>
<td style="text-align:right;">
0.80
</td>
<td style="text-align:right;">
0.49
</td>
<td style="text-align:right;">
0.49
</td>
</tr>
<tr>
<td style="text-align:right;">
0.81
</td>
<td style="text-align:right;">
0.81
</td>
<td style="text-align:right;">
0.52
</td>
<td style="text-align:right;">
0.53
</td>
</tr>
</tbody>
</table>
<pre class="r"><code>neuro_fem = neuroticism %&gt;% filter(bfi$gender==1)
neuro_male = neuroticism %&gt;% filter(bfi$gender==2)
neuro_fem_alpha = psych::alpha(neuro_fem)$total$std.alpha
neuro_male_alpha = psych::alpha(neuro_male)$total$std.alpha

alpha.ci(neuro_fem_alpha,
         n.obs = nrow(neuro_fem),
         n.var = ncol(neuro_fem)) %&gt;% 
  rbind(alpha.ci(neuro_male_alpha,
         n.obs = nrow(neuro_male),
         n.var = ncol(neuro_male)) )</code></pre>
<div id="the-uncertainty-of-alpha" class="section level4">
<h4>The Uncertainty of <span class="math inline">\(\alpha\)</span></h4>
<p>One issue with Coefficient <span class="math inline">\(\alpha\)</span> is that the uncertainty in the estimate is rarely reported, even though it has been known for some time how to derive a confidence interval for it, and tools are readily available for producing it. The <span class="pack">MBESS</span> package does this in a variety ways. One uses an approach noted in Feldt et al. <span class="citation">@feldt_statistical_1987</span>, and which assumes fixed, rather than random, items and subjects. Another is based on a normal distribution approximation <span class="citation">@van_zyl_distribution_2000</span>. The other method is via the bootstrap <span class="citation">@kelley_confidence_2016</span>, calculating <span class="math inline">\(\alpha\)</span> for <span class="math inline">\(R\)</span> number of bootstrap resamples of the data. We will use the latter for our purposes. Results are shown below, with the bootstrapped value based on 1000 iterations.</p>
<pre class="r"><code>library(MBESS)</code></pre>
<pre><code>## 
## Attaching package: &#39;MBESS&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:psych&#39;:
## 
##     cor2cov</code></pre>
<pre class="r"><code>set.seed(123)

# note that this is for raw, not standardized alpha
alpha_ci_boot   = ci.reliability(neuroticism, type = &#39;alpha&#39;, interval.type = &#39;perc&#39;, B = 1000)
alpha_ci_feldt  = ci.reliability(neuroticism, type = &#39;alpha&#39;, interval.type = &#39;feldt&#39;)
alpha_ci_normal = ci.reliability(neuroticism, type = &#39;alpha&#39;, interval.type = &#39;ml&#39;)</code></pre>
<pre class="r"><code># alpha_ci_results = list(alpha_ci_boot, alpha_ci_feldt, alpha_ci_normal) %&gt;% 
#   map_df(as_tibble)

# alpha_ci_results = alpha_ci_results %&gt;% 
#   select(interval.type, ci.lower, est, ci.upper) %&gt;% 
#   mutate(interval.type = c(&#39;boot&#39;, &#39;feldt&#39;, &#39;normal&#39;)) %&gt;% 
#   rename(` ` = interval.type,
#          LL  = ci.lower,
#          alpha = est,
#          UL = ci.upper)

alpha_ci_results = as.tibble(alpha_ci_boot)</code></pre>
<pre><code>## Warning: `as.tibble()` is deprecated, use `as_tibble()` (but mind the new semantics).
## This warning is displayed once per session.</code></pre>
<pre class="r"><code>alpha_ci_results = alpha_ci_results %&gt;% 
  select(interval.type, ci.lower, est, ci.upper) %&gt;% 
  mutate(interval.type = &#39;bootstrapped estimate&#39;) %&gt;% 
  rename(` ` = interval.type,
         LL  = ci.lower,
         alpha = est,
         UL = ci.upper) 

alpha_ci_results  %&gt;% 
  kable_df(digits = 4)</code></pre>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
LL
</th>
<th style="text-align:right;">
alpha
</th>
<th style="text-align:right;">
UL
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
bootstrapped estimate
</td>
<td style="text-align:right;">
0.8004
</td>
<td style="text-align:right;">
0.8133
</td>
<td style="text-align:right;">
0.8253
</td>
</tr>
</tbody>
</table>
<p>We can now see that our estimate of <span class="math inline">\(\alpha\)</span> would best be summarized as some value between 0.80 and 0.83.</p>
<div id="a-bayesian-approach" class="section level5">
<h5>A Bayesian Approach</h5>
<p>An alternative approach to estimating the uncertainty in <span class="math inline">\(\alpha\)</span> would be a Bayesian estimate. We could estimate the value by first estimating the correlation matrix underlying the assumed multivariate normal distribution of the observations/items. Thus the Bayesian <span class="math inline">\(\alpha\)</span> would be based on the posterior predictive distribution given the estimate of the correlation matrix <span class="citation">@padilla_estimating_2011</span>. Alternatively, we could use a normal approximation for the distribution of the <span class="math inline">\(\alpha\)</span> itself, based on the estimated correlation matrix <span class="citation">@van_zyl_distribution_2000</span>. Yet another approach would be based on a mixed model, calculating an <span class="emph">intra-class correlation coefficent</span> for a set number of items, as in Generalizability theory. We will save that for its own section later. For now, we will use the first estimate. More detail can be found in the supplemental materials.</p>
<pre class="r"><code>library(rstan); library(tidybayes)</code></pre>
<pre><code>## Loading required package: StanHeaders</code></pre>
<pre><code>## rstan (Version 2.18.2, GitRev: 2e1f913d3ca3)</code></pre>
<pre><code>## For execution on a local, multicore CPU with excess RAM we recommend calling
## options(mc.cores = parallel::detectCores()).
## To avoid recompilation of unchanged Stan programs, we recommend calling
## rstan_options(auto_write = TRUE)</code></pre>
<pre><code>## 
## Attaching package: &#39;rstan&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:psych&#39;:
## 
##     lookup</code></pre>
<pre><code>## The following object is masked from &#39;package:tidyr&#39;:
## 
##     extract</code></pre>
<pre><code>## NOTE: As of tidybayes version 1.0, several functions, arguments, and output column names
##       have undergone significant name changes in order to adopt a unified naming scheme.
##       See help(&#39;tidybayes-deprecated&#39;) for more information.</code></pre>
<pre class="r"><code>neuroticism_no_na = na.omit(scale(neuroticism))

data_list = list(X=neuroticism_no_na, 
                 N=nrow(neuroticism_no_na), 
                 p=ncol(neuroticism_no_na),
                 scale_flag = 0)

alpha_bayes = stan(file = &#39;code/alpha.stan&#39;, data = data_list, cores = 4, thin = 4)</code></pre>
<pre class="r"><code># move to technical section
alpha_draws = spread_draws(alpha_bayes, alpha, alpha2, theta)

alpha_bayes_ci = 
  alpha_draws %&gt;% 
  tidybayes::mean_qi() %&gt;% 
  select(-.width, -.point, -.interval) %&gt;% 
  tidyext::gather_multi(key     = value, 
                        values  = vars(`normal bayes`, `post. pred.`, `post. pred. non-normal`),
                        varlist = vars(c(alpha, alpha.lower, alpha.upper), starts_with(&#39;alpha2&#39;), starts_with(&#39;theta&#39;))) 

alpha_bayes_ci = as_data_frame(t(alpha_bayes_ci[,-1])) %&gt;% 
  mutate(` ` = c(&#39;normal bayes&#39;, &#39;post. pred.&#39;, &#39;post. pred. non-normal&#39;)) %&gt;% 
  select(` `, everything()) %&gt;% 
  rename(alpha = V1,
         LL = V2,
         UL = V3) %&gt;% 
  select(` `, LL, alpha, UL)

alpha_bayes_ci %&gt;% 
  kable_df()</code></pre>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
LL
</th>
<th style="text-align:right;">
alpha
</th>
<th style="text-align:right;">
UL
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
normal bayes
</td>
<td style="text-align:right;">
0.787
</td>
<td style="text-align:right;">
0.813
</td>
<td style="text-align:right;">
0.836
</td>
</tr>
<tr>
<td style="text-align:left;">
post. pred.
</td>
<td style="text-align:right;">
0.805
</td>
<td style="text-align:right;">
0.813
</td>
<td style="text-align:right;">
0.821
</td>
</tr>
<tr>
<td style="text-align:left;">
post. pred. non-normal
</td>
<td style="text-align:right;">
0.808
</td>
<td style="text-align:right;">
0.816
</td>
<td style="text-align:right;">
0.824
</td>
</tr>
</tbody>
</table>
<p>We can also view these estimates directly. The normal approximation is wider than the other two.</p>
<pre class="r"><code>alpha_draws %&gt;%
  gather(key=alpha, value=value, -.chain, -.iteration, -.draw) %&gt;%
  qplot(x=value, geom = &#39;density&#39;, data=., 
        fill = alpha,
        color = alpha) +
  scico::scale_fill_scico_d(alpha = .5) +
  scico::scale_color_scico_d(alpha = .75) +
  theme_trueMinimal()</code></pre>
<p><img src="demo_files/figure-html/vis_alpha_bayes-1.png" width="672" /></p>
<p>Here are all the estimates of uncertainty calculated. For this amount of data it is not surprising that they are mostly in agreement, though the normal approximation may be a little wider.</p>
<pre class="r"><code>alpha_ci_results %&gt;% 
  bind_rows(alpha_bayes_ci) %&gt;% 
  kable_df(digits = 2)</code></pre>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
LL
</th>
<th style="text-align:right;">
alpha
</th>
<th style="text-align:right;">
UL
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
bootstrapped estimate
</td>
<td style="text-align:right;">
0.80
</td>
<td style="text-align:right;">
0.81
</td>
<td style="text-align:right;">
0.83
</td>
</tr>
<tr>
<td style="text-align:left;">
normal bayes
</td>
<td style="text-align:right;">
0.79
</td>
<td style="text-align:right;">
0.81
</td>
<td style="text-align:right;">
0.84
</td>
</tr>
<tr>
<td style="text-align:left;">
post. pred.
</td>
<td style="text-align:right;">
0.80
</td>
<td style="text-align:right;">
0.81
</td>
<td style="text-align:right;">
0.82
</td>
</tr>
<tr>
<td style="text-align:left;">
post. pred. non-normal
</td>
<td style="text-align:right;">
0.81
</td>
<td style="text-align:right;">
0.82
</td>
<td style="text-align:right;">
0.82
</td>
</tr>
</tbody>
</table>
<pre class="r"><code>res = alpha_ci_results %&gt;% 
  bind_rows(alpha_bayes_ci) %&gt;% 
  rename(Coefficient = ` `,
         value = alpha,
         ui_l = LL,
         ui_u = UL) 

visibly:::plot_coefs(res,
                     palette = &#39;acton&#39;,
                     ref_line = alpha_results$total$raw_alpha,
                     trans = NULL) +
  labs(y=&#39;&#39;)</code></pre>
<p><img src="demo_files/figure-html/alpha_all-1.png" width="672" /></p>
</div>
</div>
<div id="simulated-data" class="section level4">
<h4>Simulated Data</h4>
<p>The simulated data allows for us to have a more controlled exploration. We know the items are multivariate normal and unidimensional, so this is where <span class="math inline">\(\alpha\)</span> shines as a measure of reliability. However, one assumption with coefficient <span class="math inline">\(\alpha\)</span> is that the loadings for such a model are equivalent, and we know they aren’t in this case. As such, coefficient <span class="math inline">\(\alpha\)</span> is an estimate of the lower bound of reliability <span class="citation">@revelle_coefficients_2009</span> This scenario will serve as a comparison when we actually look at factor analytic approaches to reliability estimation later.</p>
<p>For now, we’ll skip the formality and cut right to the chase. Here are all the previous estimates shown for this data set. The <span class="math inline">\(\alpha\)</span> is lower for this particular data, but in general we’re seeing the same thing.</p>
<pre class="r"><code>alpha_results_con = psych::alpha(data_congeneric)</code></pre>
<pre class="r"><code>library(MBESS)

set.seed(123)

alpha_ci_boot_con   = ci.reliability(data_congeneric, type = &#39;alpha&#39;, interval.type = &#39;perc&#39;, B = 1000)
alpha_ci_feldt_con  = ci.reliability(data_congeneric, type = &#39;alpha&#39;, interval.type = &#39;feldt&#39;)
alpha_ci_normal_con = ci.reliability(data_congeneric, type = &#39;alpha&#39;, interval.type = &#39;ml&#39;)</code></pre>
<pre class="r"><code>data_list = list(X=data_congeneric, 
                 N=nrow(data_congeneric), 
                 p=ncol(data_congeneric),
                 scale_flag = 0)

alpha_bayes_con = stan(file = &#39;code/alpha.stan&#39;, data = data_list, cores = 4, thin = 4)</code></pre>
<pre class="r"><code>alpha_ci_results_con = list(alpha_ci_boot_con, alpha_ci_feldt_con, alpha_ci_normal_con) %&gt;% 
  map_df(as_tibble)


alpha_ci_results_con = alpha_ci_results_con %&gt;% 
  select(interval.type, ci.lower, est, ci.upper) %&gt;% 
  mutate(interval.type = c(&#39;boot&#39;, &#39;feldt&#39;, &#39;normal&#39;)) %&gt;% 
  rename(` ` = interval.type,
         LL  = ci.lower,
         alpha = est,
         UL = ci.upper) 

alpha_draws = spread_draws(alpha_bayes_con, alpha, alpha2, theta)

alpha_bayes_ci_con = 
  alpha_draws %&gt;% 
  tidybayes::mean_qi() %&gt;% 
  select(-.width, -.point, -.interval) %&gt;% 
  tidyext::gather_multi(key     = value, 
                        values  = vars(`Normal approx.`, `Post. Pred`, `Post. Pred. Non-normal`),
                        varlist = vars(c(alpha, alpha.lower, alpha.upper), starts_with(&#39;alpha2&#39;), starts_with(&#39;theta&#39;))) 

alpha_bayes_ci_con = as_data_frame(t(alpha_bayes_ci_con[,-1])) %&gt;% 
  mutate(` ` = c(&#39;normal bayes&#39;, &#39;post. pred.&#39;, &#39;post. pred. non-normal&#39;)) %&gt;% 
  select(` `, everything()) %&gt;% 
  rename(alpha = V1,
         LL = V2,
         UL = V3) %&gt;% 
  select(` `, LL, alpha, UL)

alpha_compare = alpha_ci_results_con %&gt;% 
  bind_rows(alpha_bayes_ci_con)

alpha_compare %&gt;% kable_df(digits = 2)</code></pre>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
LL
</th>
<th style="text-align:right;">
alpha
</th>
<th style="text-align:right;">
UL
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
boot
</td>
<td style="text-align:right;">
0.69
</td>
<td style="text-align:right;">
0.72
</td>
<td style="text-align:right;">
0.75
</td>
</tr>
<tr>
<td style="text-align:left;">
feldt
</td>
<td style="text-align:right;">
0.69
</td>
<td style="text-align:right;">
0.72
</td>
<td style="text-align:right;">
0.75
</td>
</tr>
<tr>
<td style="text-align:left;">
normal
</td>
<td style="text-align:right;">
0.69
</td>
<td style="text-align:right;">
0.72
</td>
<td style="text-align:right;">
0.75
</td>
</tr>
<tr>
<td style="text-align:left;">
normal bayes
</td>
<td style="text-align:right;">
0.67
</td>
<td style="text-align:right;">
0.72
</td>
<td style="text-align:right;">
0.76
</td>
</tr>
<tr>
<td style="text-align:left;">
post. pred.
</td>
<td style="text-align:right;">
0.69
</td>
<td style="text-align:right;">
0.72
</td>
<td style="text-align:right;">
0.74
</td>
</tr>
<tr>
<td style="text-align:left;">
post. pred. non-normal
</td>
<td style="text-align:right;">
0.71
</td>
<td style="text-align:right;">
0.73
</td>
<td style="text-align:right;">
0.75
</td>
</tr>
</tbody>
</table>
<pre class="r"><code>res = alpha_compare %&gt;% 
  rename(Coefficient = ` `,
         value = alpha,
         ui_l = LL,
         ui_u = UL) 

visibly:::plot_coefs(res,
                     palette = &#39;bilbao&#39;,
                     ref_line = alpha_results_con$total$raw_alpha,
                     trans = NULL) +
  labs(y=&#39;&#39;)</code></pre>
<p><img src="demo_files/figure-html/alpha_compare_congeneric-1.png" width="672" /></p>
</div>
<div id="comparison-to-small-sample" class="section level4">
<h4>Comparison to small sample</h4>
<pre class="r"><code>neuro_10 = neuroticism %&gt;% sample_frac(.1)
neuro_05 = neuroticism %&gt;% sample_frac(.05)

alpha_ci_boot_small  = ci.reliability(neuro_10, type = &#39;alpha&#39;, interval.type = &#39;perc&#39;, B = 1000)

alpha_ci_boot_smaller  = ci.reliability(neuro_05, type = &#39;alpha&#39;, interval.type = &#39;perc&#39;, B = 1000)


alpha_ci_boot_small %&gt;% 
  bind_rows(alpha_ci_boot_smaller) %&gt;% 
  as_data_frame() %&gt;% 
  select(interval.type, ci.lower, est, ci.upper) %&gt;% 
  rename(
    Size = interval.type,
    LL  = ci.lower,
    alpha = est,
    UL = ci.upper)  %&gt;% 
  mutate(Size = c(&#39;10%&#39;, &#39;5%&#39;)) %&gt;% 
  kable_df()</code></pre>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
Size
</th>
<th style="text-align:right;">
LL
</th>
<th style="text-align:right;">
alpha
</th>
<th style="text-align:right;">
UL
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
10%
</td>
<td style="text-align:right;">
0.816
</td>
<td style="text-align:right;">
0.848
</td>
<td style="text-align:right;">
0.874
</td>
</tr>
<tr>
<td style="text-align:left;">
5%
</td>
<td style="text-align:right;">
0.793
</td>
<td style="text-align:right;">
0.841
</td>
<td style="text-align:right;">
0.878
</td>
</tr>
</tbody>
</table>
</div>
<div id="multidimensional-scale" class="section level4">
<h4>Multidimensional Scale</h4>
<p>Notes: can’t generalize from subscale to whole or vice versa, alpha &lt; reliability in this setting <span class="citation">@zinbarg_cronbachs_2005</span></p>
<pre class="r"><code>bfi_items_only = visibly::bfi
alpha_bfi_whole_scale = psych::alpha(bfi_items_only)</code></pre>
<pre><code>## Warning in psych::alpha(bfi_items_only): Some items were negatively correlated with the total scale and probably 
## should be reversed.  
## To do this, run the function again with the &#39;check.keys=TRUE&#39; option</code></pre>
<pre><code>## Some items ( N1 N2 N3 N4 N5 O4 ) were negatively correlated with the total scale and 
## probably should be reversed.  
## To do this, run the function again with the &#39;check.keys=TRUE&#39; option</code></pre>
<pre class="r"><code>alpha_neuro = psych::alpha(neuroticism)
alpha_extra = psych::alpha(select(bfi_items_only, 
                                  starts_with(&#39;E&#39;, ignore.case = F)))
alpha_open  = psych::alpha(select(bfi_items_only, 
                                  starts_with(&#39;O&#39;, ignore.case = F)))
alpha_agree = psych::alpha(select(bfi_items_only, 
                                  starts_with(&#39;A&#39;, ignore.case = F)))
alpha_consc = psych::alpha(select(bfi_items_only, 
                                  starts_with(&#39;C&#39;, ignore.case = F)))

alpha_bfi_list = list(alpha_neuro, 
                      alpha_extra, 
                      alpha_open, 
                      alpha_agree, 
                      alpha_consc)

alpha_bfi_indiv_scale = alpha_bfi_list %&gt;% 
  map(function(x) x$total$raw[1]) %&gt;% 
  cbind() %&gt;% 
  as.data.frame() %&gt;% 
  mutate(Scale = c(&#39;Neuro&#39;, &#39;Extra&#39;, &#39;Open&#39;, &#39;Agree&#39;, &#39;Consc&#39;)) %&gt;% 
  rename(Alpha = &#39;.&#39;) %&gt;% 
  select(Scale, Alpha) %&gt;% 
  unnest()

alpha_bfi_whole_scale$total %&gt;% 
  as_data_frame() %&gt;% 
  rename(`Raw` = raw_alpha,
         Standardized = std.alpha,
         G6 = `G6(smc)`,
         `Avg. Inter-item cor` = average_r,
         ASE = ase,
         `Median r` = median_r) %&gt;% 
  select(Raw, Standardized, `Avg. Inter-item cor`, `Median r`, ASE) %&gt;% 
  kable_df(digits = 2)</code></pre>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
Raw
</th>
<th style="text-align:right;">
Standardized
</th>
<th style="text-align:right;">
Avg. Inter-item cor
</th>
<th style="text-align:right;">
Median r
</th>
<th style="text-align:right;">
ASE
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
0.69
</td>
<td style="text-align:right;">
0.71
</td>
<td style="text-align:right;">
0.09
</td>
<td style="text-align:right;">
0.08
</td>
<td style="text-align:right;">
0.01
</td>
</tr>
</tbody>
</table>
<pre class="r"><code>alpha_bfi_indiv_scale %&gt;% 
  kable_df()</code></pre>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
Scale
</th>
<th style="text-align:right;">
Alpha
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Neuro
</td>
<td style="text-align:right;">
0.813
</td>
</tr>
<tr>
<td style="text-align:left;">
Extra
</td>
<td style="text-align:right;">
0.762
</td>
</tr>
<tr>
<td style="text-align:left;">
Open
</td>
<td style="text-align:right;">
0.600
</td>
</tr>
<tr>
<td style="text-align:left;">
Agree
</td>
<td style="text-align:right;">
0.703
</td>
</tr>
<tr>
<td style="text-align:left;">
Consc
</td>
<td style="text-align:right;">
0.727
</td>
</tr>
</tbody>
</table>
</div>
<div id="limitations-of-coefficient-alpha" class="section level4">
<h4>Limitations of Coefficient <span class="math inline">\(\alpha\)</span></h4>
<ul>
<li>Assumes an underlying single factor model with equal loadings for items</li>
<li>Unless the assumptions, <span class="math inline">\(\alpha\)</span> can only provide a lower bound estimate</li>
<li>Merely adding items will improve the estimate, which may not be how we think about reliability for a given scenario</li>
<li>Internal consistency can be low even measures are stable across time; see <code>?psych::epiR</code> for example.</li>
</ul>
</div>
</div>
<div id="generalizability-theory" class="section level3">
<h3>Generalizability theory</h3>
<p>So called <span class="emph">mixed effects models</span> are statistical models applicable to situations in which there is some dependency among observations in the data, where that correlation typically arises from the observations being clustered in some way. For example, it is quite common to have data in which we have repeated measurements of individuals, or cases in which the units of observation are otherwise grouped together, for example, students within school, or cities within geographic region. This clustering can be hierarchical in nature (e.g. students within schools, schools within districts) or not (e.g. students and items on a test). While there are different ways to approach such a situation, mixed models are a powerful tool with which to do so.</p>
<p>Mixed models estimate the variance attributable to the various sources of dependency in the data. Thus, aside from the usual regression output, we get a sense of variability due to individuals, species, surgical procedure, or whatever our grouping structure is. In addition, we may estimate cluster-specific effects, which allow for increased predictive capability. For example, we may know the general trend across all individuals, but we can also allow each individual to have separate starting points and trends.</p>
<p>In our data example, each person sees the five items, so those scores within each person are not independent. In other words, the multiple observations are clustered within individuals. This becomes more clear when we consider our data in ‘long’ format, as follows.</p>
<pre class="r"><code>neuroticism_long = neuroticism %&gt;% 
  rowid_to_column(var=&#39;person&#39;) %&gt;%          # create person id
  gather(key = item, value=score, -person)   # melt data into long format</code></pre>
<pre class="r"><code>neuroticism_long %&gt;% 
  arrange(person, item) %&gt;% 
  head(10) %&gt;%
  kable_df()</code></pre>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
person
</th>
<th style="text-align:left;">
item
</th>
<th style="text-align:right;">
score
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
N1
</td>
<td style="text-align:right;">
3
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
N2
</td>
<td style="text-align:right;">
4
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
N3
</td>
<td style="text-align:right;">
2
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
N4
</td>
<td style="text-align:right;">
2
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
N5
</td>
<td style="text-align:right;">
3
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:left;">
N1
</td>
<td style="text-align:right;">
3
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:left;">
N2
</td>
<td style="text-align:right;">
3
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:left;">
N3
</td>
<td style="text-align:right;">
3
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:left;">
N4
</td>
<td style="text-align:right;">
5
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:left;">
N5
</td>
<td style="text-align:right;">
5
</td>
</tr>
</tbody>
</table>
<p>In similar fashion, we can see observations within items as being more correlated than observations within other items. As such, in this case we can treat the person or item as a source of variance, or in other settings, even their interaction.</p>
<p><span class="emph">Generalizability Theory</span> focuses on the variance components resulting from a linear mixed model. Using the variance estimates we can obtain a measure of the reliability of a mean score for an individual across the multiple observations. This reliability is defined as what proportion of the total variance is attributable to the particular source of variance, e.g. person, we wish to study. For some grouping or clustering factor <span class="math inline">\(g\)</span>, the simplest estimate of reliability (<span class="math inline">\(\rho\)</span>) would be calculated as follows:</p>
<p><span class="math display">\[\rho = \frac{\sigma_g^2}{\sigma_g^2 +  \sigma_{residual}^2}\]</span></p>
<aside>
Many will recognize this as the ICC or <span class="emph">intra-class correlation coefficient</span>.
</aside>
<p>In more complicated circumstances:</p>
<p><span class="math display">\[\rho = \frac{\sigma_g^2 + \sigma_{g*}^2}{\sigma_g^2 + \sigma_{g*}^2 + \sigma_{other}^2 + \sigma_{residual}^2}\]</span></p>
<p>In the above, <span class="math inline">\(g*\)</span> refers to interactions of g with other other sources of variance, <span class="math inline">\(other\)</span> refers to other still other sources of variance that don’t include <span class="math inline">\(g*\)</span>, and then the <span class="math inline">\(residual\)</span> variance is whatever else is not accounted for.</p>
<p>For simplicity we will focus on the person variance only in our examples.</p>
<pre class="r"><code># alpha as icc/generalizability statistic from g-theory
bfi_long = bfi_items_only %&gt;% 
  gather(key = item, value = score) %&gt;% 
  mutate(scale = str_sub(item, end=1))

library(rstanarm)
mixed_congeneric_bayes = stan_lmer(value ~ (1|subject), d_congeneric_long_std, cores=4, thin=4)
VarCorr(mixed_congeneric_bayes)

# note that Sigma is var intercept, while sigma is residual sd
var_comp_draws = spread_draws(mixed_congeneric_bayes, Sigma[subject:(Intercept),(Intercept)], sigma) %&gt;% 
  mutate(sigma = sigma^2,
         alpha = Sigma/(Sigma + sigma/N_items))

# var_comp_draws %&gt;% mean_qi(alpha)
print(alpha_bayes, par=&#39;alpha&#39;, digits=4)</code></pre>
<div id="the-uncertainty-of-generalizability" class="section level4">
<h4>The Uncertainty of Generalizability</h4>
</div>
<div id="simulated-data-1" class="section level4">
<h4>Simulated Data</h4>
</div>
<div id="multidimensional-scale-1" class="section level4">
<h4>Multidimensional Scale</h4>
</div>
<div id="limitations-of-g-theory" class="section level4">
<h4>Limitations of G-theory</h4>
</div>
</div>
<div id="factor-analysis" class="section level3">
<h3>Factor Analysis</h3>
<p>omega (as generalization of alpha), ave</p>
<div id="the-uncertainty-of-factor-loadings" class="section level4">
<h4>The Uncertainty of Factor Loadings</h4>
</div>
<div id="simulated-data-2" class="section level4">
<h4>Simulated Data</h4>
</div>
<div id="multidimensional-scale-2" class="section level4">
<h4>Multidimensional Scale</h4>
</div>
<div id="limitations-of-factor-analytic-approach" class="section level4">
<h4>Limitations of Factor Analytic Approach</h4>
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Despite this, you will see it frequently reported in cases of multidimensional factor structure.<a href="#fnref1" class="footnote-back">↩</a></p></li>
</ol>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
